


There is two ways, ollama is CLI tool to run models localy. You run server `ollama serve` locally and then you run specific model which over ollama local server and epxosed API's receive inputs and do inference locally and then return values `ollama run deepkseek-1`


Second option is via jupyter lab. You run `jupyter lab` localy, and then via VS code extenstion or via browser and jupyter lab you can execute some code by using Google Gemini API. Jupyter lab will be aware of local env context in the sense of packagies as well as enviroment varibales. 


