{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
   "metadata": {},
   "source": [
    "# Gradio Day!\n",
    "\n",
    "Today we will build User Interfaces using the outrageously simple Gradio framework.\n",
    "\n",
    "Prepare for joy!\n",
    "\n",
    "Please note: your Gradio screens may appear in 'dark mode' or 'light mode' depending on your computer settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr \n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv  \n",
    "load_dotenv()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AIzaSyB4\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name='gemini-2.5-flash', \n",
    "    system_instruction=\"You are a helpful assistant that responds in markdown\",\n",
    "    generation_config={\n",
    "        'temperature': 0.3,  # Lower for more consistent responses\n",
    "        'max_output_tokens': 2048,\n",
    "    }\n",
    "    \n",
    ")\n",
    "\n",
    "def stream_gemini(prompt: str):\n",
    "    \"\"\"\n",
    "    Stream incremental output from Gemini; suitable for Gradio generator functions.\n",
    "    \"\"\"\n",
    "    stream = model.generate_content(prompt, stream=True)\n",
    "    acc = \"\"\n",
    "    for chunk in stream:\n",
    "        text = getattr(chunk, \"text\", None)\n",
    "        if text:\n",
    "            acc += text\n",
    "            yield acc\n",
    "\n",
    "\n",
    "def message_gemini(prompt: str) -> str:\n",
    "    # Uses the already-created `model` with its system_instruction\n",
    "    resp = model.generate_content(prompt)\n",
    "    return resp.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94013d1-4f27-4329-97e8-8c58db93636a",
   "metadata": {},
   "source": [
    "## User Interface time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a3262-e626-4e4b-80b0-aca152405e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message format\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f789-ff11-4cba-ac67-11b815e29d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream format\n",
    "view = gr.Interface(\n",
    "    fn=stream_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
